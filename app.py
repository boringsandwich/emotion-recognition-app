import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import os
import random
import glob
import cv2
import av
from PIL import Image

# Biblioteki do streamingu wideo w chmurze
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase, WebRtcMode

# Importy AI
import tensorflow as tf
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from deepface import DeepFace

# --- KONFIGURACJA STRONY ---
st.set_page_config(page_title="System Analizy Emocji (Hybrydowy)", layout="wide")

# ==========================================
# KONFIGURACJA 1: TW√ìJ MODEL (BADANIA)
# ==========================================
MODEL_PATH = 'moj_model_fer.h5'
DATASET_TEST_PATH = './dane_fer/test'
DEFAULT_USER_FOLDER = 'test_real'

CLASSES = ['angry', 'happy', 'sad']
TRANSLATION = {'angry': 'Z≈ÅO≈öƒÜ', 'happy': 'RADO≈öƒÜ', 'sad': 'SMUTEK'}
COLORS = {'angry': '#FF4B4B', 'happy': '#2ECC71', 'sad': '#3498DB'}

# ==========================================
# KONFIGURACJA 2: KAMERA (DEEPFACE)
# ==========================================
# Emocje, kt√≥re nas interesujƒÖ
TARGET_EMOTIONS = {'angry': 'Z≈ÅO≈öƒÜ', 'happy': 'RADO≈öƒÜ', 'sad': 'SMUTEK'}

# Kolory BGR dla OpenCV (Ramki wideo)
BOX_COLORS = {
    'Z≈ÅO≈öƒÜ': (0, 0, 255),  # Czerwony
    'RADO≈öƒÜ': (0, 255, 0),  # Zielony
    'SMUTEK': (255, 0, 0),  # Niebieski
    'NIEOKRE≈öLONY': (200, 200, 200)
}


# --- FUNKCJE POMOCNICZE (TW√ìJ MODEL) ---
@st.cache_resource
def load_ai_model():
    if not os.path.exists(MODEL_PATH): return None
    return tf.keras.models.load_model(MODEL_PATH)


def preprocess_image(image_path):
    try:
        img = load_img(image_path, color_mode='grayscale', target_size=(48, 48))
        img_array = img_to_array(img)
        img_array /= 255.0
        return np.expand_dims(img_array, axis=0)
    except:
        return None


def get_dataset_images(limit=None):
    search_path = os.path.join(DATASET_TEST_PATH, '**', '*.jpg')
    found_files = glob.glob(search_path, recursive=True)
    if not found_files: return []
    if limit and len(found_files) > limit: return random.sample(found_files, limit)
    return found_files


# ==========================================
# KLASA DO PRZETWARZANIA WIDEO (WEBRTC)
# ==========================================
class EmotionProcessor(VideoProcessorBase):
    def __init__(self):
        self.frame_count = 0
        self.last_label = "Szukam..."
        self.last_color = (255, 255, 255)
        # ≈Åadujemy detektor twarzy raz przy starcie
        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

    def recv(self, frame):
        # Konwersja klatki na format OpenCV (numpy)
        img = frame.to_ndarray(format="bgr24")

        # 1. Wykrywanie twarzy (Szybkie)
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        faces = self.face_cascade.detectMultiScale(gray, 1.1, 4)

        for (x, y, w, h) in faces:
            # Rysujemy ramkƒô
            cv2.rectangle(img, (x, y), (x + w, y + h), self.last_color, 2)

            # 2. Analiza DeepFace co 10 klatek (dla p≈Çynno≈õci)
            if self.frame_count % 10 == 0:
                try:
                    # Wycinamy twarz
                    face_roi = img[y:y + h, x:x + w]

                    # DeepFace
                    res = DeepFace.analyze(face_roi, actions=['emotion'], enforce_detection=False)
                    if isinstance(res, list): res = res[0]

                    all_emotions = res['emotion']  # np. {'angry': 20, 'happy': 5, 'neutral': 70...}

                    # --- FILTROWANIE (TYLKO 3 EMOCJE) ---
                    # WyciƒÖgamy punkty tylko dla angry, happy, sad
                    scores = {k: all_emotions.get(k, 0) for k in TARGET_EMOTIONS.keys()}

                    total = sum(scores.values())
                    if total > 0:
                        # Znajd≈∫ tƒô, kt√≥ra ma najwiƒôcej punkt√≥w w≈õr√≥d naszej tr√≥jki
                        winner_key = max(scores, key=scores.get)
                        winner_conf = scores[winner_key] / total  # Normalizacja

                        self.last_label = f"{TARGET_EMOTIONS[winner_key]} ({winner_conf:.0%})"
                        self.last_color = BOX_COLORS.get(TARGET_EMOTIONS[winner_key], (200, 200, 200))
                    else:
                        self.last_label = "Inna emocja"
                        self.last_color = (200, 200, 200)

                except Exception:
                    pass

            # Podpis nad g≈ÇowƒÖ (zawsze, nawet jak nie analizujemy w tej klatce)
            cv2.putText(img, self.last_label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, self.last_color, 2)

        self.frame_count += 1
        return av.VideoFrame.from_ndarray(img, format="bgr24")


# =========================================================
# G≈Å√ìWNY INTERFEJS
# =========================================================

st.sidebar.title("üéõÔ∏è Panel Sterowania")
app_mode = st.sidebar.selectbox(
    "Wybierz tryb:",
    ["üìÇ Badanie (M√≥j Model - Pliki)", "üìπ Kamera (DeepFace - Live/Foto)"]
)

# ---------------------------------------------------------
# TRYB 1: BADANIE (TW√ìJ MODEL) - BEZ ZMIAN
# ---------------------------------------------------------
if app_mode == "üìÇ Badanie (M√≥j Model - Pliki)":
    st.title("üß† Badanie: Tw√≥j Model .h5")
    model = load_ai_model()
    if not model:
        st.error(f"Brak modelu {MODEL_PATH}")
        st.stop()

    source = st.sidebar.radio("≈πr√≥d≈Ço:", ("üìÇ Folder test_real", "üìö Dataset"))
    img_paths = []

    if source == "üìÇ Folder test_real":
        folder = st.sidebar.text_input("Folder:", DEFAULT_USER_FOLDER)
        if os.path.exists(folder):
            img_paths = [os.path.join(folder, f) for f in os.listdir(folder) if f.lower().endswith(('jpg', 'png'))]
    else:
        if os.path.exists(DATASET_TEST_PATH):
            limit = st.sidebar.number_input("Ile zdjƒôƒá?", 10, 500, 50)
            img_paths = get_dataset_images(limit)

    if img_paths:
        if st.button("Uruchom analizƒô"):
            results = []
            bar = st.progress(0)
            for i, p in enumerate(img_paths):
                proc = preprocess_image(p)
                if proc is not None:
                    pred = model.predict(proc, verbose=0)[0]
                    idx = np.argmax(pred)
                    label = CLASSES[idx]

                    # Ground Truth
                    folder_name = os.path.basename(os.path.dirname(p))
                    true_lbl = TRANSLATION.get(folder_name, "-") if folder_name in CLASSES else "-"

                    results.append({
                        "Plik": os.path.basename(p),
                        "Wykryta": TRANSLATION[label],
                        "Pewnosc": np.max(pred),
                        "Prawda": true_lbl,
                        "Poprawne": (true_lbl == TRANSLATION[label]) if true_lbl != "-" else False,
                        "≈öcie≈ºka": p,
                        "Raw_Angry": pred[0], "Raw_Happy": pred[1], "Raw_Sad": pred[2]
                    })
                bar.progress((i + 1) / len(img_paths))

            st.session_state['df_res'] = pd.DataFrame(results)

    if 'df_res' in st.session_state:
        df = st.session_state['df_res']
        t1, t2, t3 = st.tabs(["Raport", "PrzeglƒÖdarka", "Bayes"])

        with t1:
            st.write(f"Zanalizowano {len(df)} plik√≥w.")
            fig = px.pie(df, names='Wykryta', color='Wykryta',
                         color_discrete_map={v: COLORS[k] for k, v in TRANSLATION.items()})
            st.plotly_chart(fig)
            st.dataframe(df)

        with t2:
            st.write("Galeria (Posortowana: Poprawne -> B≈Çƒôdne)")
            df_s = df.sort_values(by=['Poprawne', 'Pewnosc'], ascending=[False, False])
            cols = st.columns(5)
            for i, row in df_s.head(20).iterrows():
                cols[i % 5].image(row['≈öcie≈ºka'], caption=f"{row['Wykryta']} ({row['Pewnosc']:.0%})")

        with t3:
            st.write("Symulacja Bayesa")
            sel = st.selectbox("Plik:", df['Plik'])
            row = df[df['Plik'] == sel].iloc[0]
            st.image(row['≈öcie≈ºka'], width=150)

            pa = st.slider("Szansa Z≈Ço≈õƒá", 0.0, 1.0, 0.33)
            ph = st.slider("Szansa Rado≈õƒá", 0.0, 1.0, 0.33)
            ps = st.slider("Szansa Smutek", 0.0, 1.0, 0.33)
            priors = np.array([pa, ph, ps])
            priors /= (priors.sum() + 1e-9)

            like = np.array([row['Raw_Angry'], row['Raw_Happy'], row['Raw_Sad']])
            post = like * priors
            post /= post.sum()

            fig = go.Figure(data=[
                go.Bar(name='Model', x=list(TRANSLATION.values()), y=like),
                go.Bar(name='Bayes', x=list(TRANSLATION.values()), y=post)
            ])
            st.plotly_chart(fig)

# ---------------------------------------------------------
# TRYB 2: KAMERA (LIVE + FOTO) - WEBRTC & DEEPFACE
# ---------------------------------------------------------
elif app_mode == "üìπ Kamera (DeepFace - Live/Foto)":
    st.title("üìπ Detekcja Live (WebRTC)")
    st.markdown("Wybierz metodƒô. **Live Stream** mo≈ºe chwilƒô ≈Çadowaƒá siƒô na starcie.")

    method = st.radio("Metoda:", ["üî¥ Live Stream (Wideo)", "üì∏ Pojedyncze Zdjƒôcie"])

    if method == "üî¥ Live Stream (Wideo)":
        st.write("Kliknij **START**, aby uruchomiƒá kamerƒô. Zezw√≥l przeglƒÖdarce na dostƒôp.")

        # To jest poprawiony fragment w sekcji: elif app_mode == "üìπ Kamera (DeepFace - Live/Foto)":

        # Definicja serwer√≥w STUN (niezbƒôdne, aby obraz przeszed≈Ç przez sieƒá w chmurze)
        RTC_CONFIGURATION = {
            "iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]
        }

        webrtc_streamer(
            key="emotion-filter",
            mode=WebRtcMode.SENDRECV,
            video_processor_factory=EmotionProcessor,
            rtc_configuration=RTC_CONFIGURATION,  # <--- DODANO TO
            media_stream_constraints={"video": True, "audio": False},
            async_processing=True
        )
        st.info("üí° Wideo mo≈ºe mieƒá op√≥≈∫nienie, poniewa≈º analiza DeepFace jest wymagajƒÖca obliczeniowo.")

    else:
        # Tryb zdjƒôcia (Stary dobry camera_input)
        img_buffer = st.camera_input("Zr√≥b zdjƒôcie")
        if img_buffer:
            temp = "temp.jpg"
            with open(temp, "wb") as f:
                f.write(img_buffer.getbuffer())

            col1, col2 = st.columns(2)
            col1.image(temp)

            with st.spinner("Analiza..."):
                try:
                    res = DeepFace.analyze(temp, actions=['emotion'], enforce_detection=False)
                    if isinstance(res, list): res = res[0]

                    all_emotions = res['emotion']
                    # Filtrowanie 3 emocji
                    scores = {k: all_emotions.get(k, 0) for k in TARGET_EMOTIONS.keys()}
                    total = sum(scores.values())

                    if total > 0:
                        norm_scores = {k: v / total for k, v in scores.items()}
                        winner = max(norm_scores, key=norm_scores.get)

                        col2.success(f"Wynik: **{TARGET_EMOTIONS[winner]}**")
                        col2.metric("Pewno≈õƒá", f"{norm_scores[winner]:.1%}")

                        # Wykres
                        df_chart = pd.DataFrame({
                            'Emocja': [TARGET_EMOTIONS[k] for k in norm_scores],
                            'Wynik': list(norm_scores.values())
                        })
                        fig = px.bar(df_chart, x='Emocja', y='Wynik', color='Emocja',
                                     color_discrete_map={'Z≈ÅO≈öƒÜ': '#FF4B4B', 'RADO≈öƒÜ': '#2ECC71', 'SMUTEK': '#3498DB'})
                        col2.plotly_chart(fig)
                    else:
                        col2.warning("Wykryto twarz, ale ≈ºadna z emocji (Z≈Ço≈õƒá/Rado≈õƒá/Smutek) nie jest dominujƒÖca.")

                except Exception as e:
                    st.error(f"B≈ÇƒÖd: {e}")